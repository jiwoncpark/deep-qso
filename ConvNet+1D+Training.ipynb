{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thanks to Burak Himmetoglu at\n",
    "# https://github.com/healthDataScience/deep-learning-HAR\n",
    "# for providing the framework\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os, sys\n",
    "#from sklearn.model_selection import train_test_split\n",
    "data_path = os.path.join(os.environ['DEEPQSODIR'], 'data')\n",
    "sys.path.insert(0, data_path)\n",
    "from data_utils import *\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from sklearn.model_selection import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quasar classification using a 1D ConvNet\n",
    "### Author: Ji Won Park (jiwoncpark)\n",
    "In this notebook, we perform quasar classification using a fairly shallow 1D ConvNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "features_path = os.path.join(data_path, 'features.npy')\n",
    "label_path = os.path.join(data_path, 'labels.npy')\n",
    "\n",
    "# Load data\n",
    "X = np.load(features_path)\n",
    "y = np.load(label_path).reshape(-1).astype(int)\n",
    "\n",
    "# Directory where model checkpoints (weights) will be stored\n",
    "weights_dir = 'checkpoints-cnn'\n",
    "if (os.path.exists(weights_dir) == False):\n",
    "    os.makedirs(weights_dir)\n",
    "    \n",
    "# Directory where logs will be stored\n",
    "logs_dir = 'logs-cnn'\n",
    "if (os.path.exists(logs_dir) == False):\n",
    "    os.makedirs(logs_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data constants\n",
    "NUM_CLASSES = 2\n",
    "NUM_OBJECTS, NUM_TIMES, NUM_CHANNELS = X.shape\n",
    "\n",
    "# Training hyperparameters\n",
    "DEBUG = True\n",
    "BATCH_SIZE = 500\n",
    "LEARNING_RATE = 1.e-2\n",
    "NUM_EPOCHS = 2\n",
    "KEEP_PROB = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up k-fold cross validation\n",
    "kf = KFold(n_splits=3, shuffle=True, random_state=123)\n",
    "\n",
    "# One-hot encode labels\n",
    "y = to_onehot(y, num_classes=NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up graph\n",
    "graph = tf.Graph()\n",
    "\n",
    "# Construct placeholders\n",
    "with graph.as_default():\n",
    "    inputs_ = tf.placeholder(tf.float32, [None, NUM_TIMES, NUM_CHANNELS], name = 'inputs')\n",
    "    labels_ = tf.placeholder(tf.float32, [None, NUM_CLASSES], name = 'labels')\n",
    "    keep_prob_ = tf.placeholder(tf.float32, name = 'keep')\n",
    "    learning_rate_ = tf.placeholder(tf.float32, name = 'learning_rate')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After conv0:  (?, 738, 8)\n",
      "After conv00, max_pool00:  (?, 369, 16)\n",
      "After conv1, max_pool_1:  (?, 123, 48)\n",
      "After conv2, max_pool_2:  (?, 41, 144)\n"
     ]
    }
   ],
   "source": [
    "# Define network architecture\n",
    "with graph.as_default():\n",
    "    with tf.name_scope('conv0'):\n",
    "        # (batch, 738, NUM_CHANNELS) --> (batch, 738, NUM_CHANNELS)\n",
    "        conv0 = tf.layers.conv1d(inputs=inputs_, filters=NUM_CHANNELS, kernel_size=1, strides=1,\n",
    "                                 padding='same', activation = tf.nn.relu, name='conv0')\n",
    "        if DEBUG: print(\"After conv0: \", conv0.shape)\n",
    "\n",
    "    with tf.name_scope('conv_mp0'):\n",
    "        # (batch, 738, NUM_CHANNELS) --> (batch, 369, NUM_CHANNELS*2)\n",
    "        conv00 = tf.layers.conv1d(inputs=conv0, filters=NUM_CHANNELS*2, kernel_size=1, strides=1,\n",
    "                                  padding='same', activation = tf.nn.relu, name='conv00')\n",
    "        max_pool_00 = tf.layers.max_pooling1d(inputs=conv00, pool_size=2, strides=2, padding='same', name='maxpool00')\n",
    "        if DEBUG: print(\"After conv00, max_pool00: \", max_pool_00.shape)\n",
    "\n",
    "    with tf.name_scope('conv_mp1'):\n",
    "        # (batch, 369, NUM_CHANNELS*2) --> (batch, 123, NUM_CHANNELS*2*3)\n",
    "        conv1 = tf.layers.conv1d(inputs=max_pool_00, filters=NUM_CHANNELS*2*3, kernel_size=2, strides=1, \n",
    "                                 padding='same', activation = tf.nn.relu, name='conv1')\n",
    "        max_pool_1 = tf.layers.max_pooling1d(inputs=conv1, pool_size=2, strides=3, padding='same', name='maxpool1')\n",
    "        if DEBUG: print(\"After conv1, max_pool_1: \", max_pool_1.shape)\n",
    "    \n",
    "    with tf.name_scope('conv_mp2'):\n",
    "        # (batch, 123, NUM_CHANNELS*2*3) --> (batch, 41, NU_CHANNELS*2*3*3)\n",
    "        conv2 = tf.layers.conv1d(inputs=max_pool_1, filters=NUM_CHANNELS*2*3*3, kernel_size=2, strides=1, \n",
    "                                 padding='same', activation = tf.nn.relu, name='conv2')\n",
    "        max_pool_2 = tf.layers.max_pooling1d(inputs=conv2, pool_size=2, strides=3, padding='same', name='maxpool2')\n",
    "        if DEBUG: print(\"After conv2, max_pool_2: \", max_pool_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define cost function and metrics\n",
    "with graph.as_default():\n",
    "    with tf.name_scope('fc'):\n",
    "        flat = tf.reshape(max_pool_2, (-1, 41*NUM_CHANNELS*2*3*3))\n",
    "    with tf.name_scope('dropout'):\n",
    "        flat = tf.nn.dropout(flat, keep_prob=keep_prob_)\n",
    "    \n",
    "    # Predictions\n",
    "    logits = tf.layers.dense(flat, NUM_CLASSES)\n",
    "    \n",
    "    with tf.name_scope('cost'):\n",
    "        cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=labels_))\n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate_).minimize(cost)\n",
    "    \n",
    "    with tf.name_scope('accuracy'):\n",
    "        labels_1d = tf.argmax(labels_, axis=1)\n",
    "        preds_1d = tf.argmax(logits, axis=1)\n",
    "        correct_pred = tf.equal(preds_1d, labels_1d)\n",
    "        accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "    \n",
    "    with tf.name_scope('confusion_matrix'):\n",
    "        conf_matrix = tf.confusion_matrix(labels=labels_1d, predictions= preds_1d,\n",
    "                                          num_classes=NUM_CLASSES, name='conf_matrix')\n",
    "\n",
    "    with tf.name_scope('summary'):\n",
    "        # Monitor cost\n",
    "        tf.summary.scalar(\"loss\", cost)\n",
    "        # Monitor accuracy\n",
    "        tf.summary.scalar(\"accuracy\", accuracy)\n",
    "        # Monitor FP\n",
    "        tf.summary.scalar(\"false_positives\", conf_matrix[0, 1])\n",
    "        # Monitor FN\n",
    "        tf.summary.scalar(\"false_negatives\", conf_matrix[1, 0])\n",
    "        # Merge all summaries into a single op\n",
    "        merged = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with graph.as_default():\n",
    "    saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/2 Learning rate: 0.010000 Iteration: 43 Validation loss: 0.462525 Validation acc: 0.997505\n",
      "[[5285    2]\n",
      " [  24 5110]]\n",
      "Epoch: 1/2 Learning rate: 0.010000 Iteration: 85 Validation loss: 0.011987 Validation acc: 0.999904\n",
      "[[5147    1]\n",
      " [   0 5273]]\n",
      "Epoch: 1/2 Learning rate: 0.010000 Iteration: 127 Validation loss: 0.003887 Validation acc: 0.999808\n",
      "[[5194    2]\n",
      " [   0 5224]]\n",
      "CV mean accuracy: 0.999072, CV std: 0.00110899\n",
      "CV mean loss: 0.159466\n",
      "Epoch: 2/2 Learning rate: 0.010000 Iteration: 169 Validation loss: 0.002765 Validation acc: 0.999520\n",
      "[[5282    5]\n",
      " [   0 5134]]\n",
      "Epoch: 2/2 Learning rate: 0.010000 Iteration: 211 Validation loss: 0.002825 Validation acc: 0.999520\n",
      "[[5143    5]\n",
      " [   0 5273]]\n",
      "Epoch: 2/2 Learning rate: 0.010000 Iteration: 253 Validation loss: 0.001438 Validation acc: 0.999808\n",
      "[[5194    2]\n",
      " [   0 5224]]\n",
      "CV mean accuracy: 0.999616, CV std: 0.000135713\n",
      "CV mean loss: 0.00234251\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "train_acc, train_loss = [], []\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    summary_writer = tf.summary.FileWriter(logs_dir, graph=sess.graph)\n",
    "    iteration = 1\n",
    "    # Loop over epochs\n",
    "    for e in range(1, NUM_EPOCHS+1):\n",
    "        cv_valacc, cv_valloss = [], []\n",
    "        for train_index, val_index in kf.split(X):\n",
    "            X_train, X_val = X[train_index], X[val_index]\n",
    "            y_train, y_val = y[train_index], y[val_index]            \n",
    "                \n",
    "            # Loop over batches\n",
    "            for x_t, y_t in fetch_batches(X_train, y_train, batch_size=BATCH_SIZE):\n",
    "                # Feed dictionary\n",
    "                feed = {inputs_ : x_t,\n",
    "                        labels_ : y_t, \n",
    "                        keep_prob_ : KEEP_PROB, \n",
    "                        learning_rate_ : LEARNING_RATE}\n",
    "                # Run ops\n",
    "                loss, _ , acc, cm, summary = sess.run([cost, optimizer, accuracy, conf_matrix, merged], \n",
    "                                                      feed_dict = feed)\n",
    "                # Add summary\n",
    "                summary_writer.add_summary(summary, iteration)\n",
    "                train_acc.append(acc)\n",
    "                train_loss.append(loss)\n",
    "                iteration += 1\n",
    "\n",
    "            # Compute validation loss at the end of every CV epoch\n",
    "            val_acc_ = []\n",
    "            val_loss_ = []\n",
    "\n",
    "            #for x_v, y_v in fetch_batches(X_val, y_val, BATCH_SIZE):\n",
    "            feed = {inputs_ : X_val, labels_ : y_val, keep_prob_ : 1.0}  \n",
    "            loss_v, acc_v, cm_v = sess.run([cost, accuracy, conf_matrix], feed_dict = feed)                    \n",
    "            val_acc_.append(acc_v)\n",
    "            val_loss_.append(loss_v)\n",
    "\n",
    "            print(\"Epoch: {}/{}\".format(e, NUM_EPOCHS),\n",
    "                  \"Learning rate: {:.6f}\".format(LEARNING_RATE),\n",
    "                  \"Iteration: {:d}\".format(iteration),\n",
    "                  \"Validation loss: {:6f}\".format(np.mean(val_loss_)),\n",
    "                  \"Validation acc: {:.6f}\".format(np.mean(val_acc_)))\n",
    "            print(cm_v)\n",
    "            # Store accuracy and loss\n",
    "            cv_valacc.append(np.mean(val_acc_))\n",
    "            cv_valloss.append(np.mean(val_loss_))\n",
    "        print('CV mean accuracy: {:.6}, CV std: {:.6}'.format(np.mean(cv_valacc), np.std(cv_valacc)))\n",
    "        print('CV mean loss: {:.6}'.format(np.mean(cv_valloss)))\n",
    "        saver.save(sess, os.path.join(weights_dir, 'ckpt-cnn'))\n",
    "summary_writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
